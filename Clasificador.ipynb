{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDetG_9ig91e"
      },
      "outputs": [],
      "source": [
        "NAME = \"Vargas Bautista Emmanuel\"\n",
        "COLLABORATORS = \"Me Myself and I\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4l07hkbg91f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur0usXO-g91f"
      },
      "source": [
        "# Clasificador (SOM)\n",
        "\n",
        "Clasificador</a> by <span property=\"cc:attributionName\">Miguel Angel Pérez León</span> is licensed under <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC-SA 4.0<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\"></a></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufhH0IVug91g"
      },
      "outputs": [],
      "source": [
        "# bibliotecas que se van a usar\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = \"./Textos\"\n",
        "# En caso de no existir el directorio de docs\n",
        "isExist = os.path.exists(path)\n",
        "if not isExist:\n",
        "\n",
        "    # Se crea el directorio\n",
        "    os.makedirs(path)\n",
        "\n",
        "    # Se descargan los archivos\n",
        "    for i in range(1,12):\n",
        "        nombre = 'texto'+str(i)+'.txt'\n",
        "        url = 'https://raw.githubusercontent.com/jugernaut/Induccion_MeIA/angel/utils/data/textosMike/'+nombre\n",
        "        response = requests.get(url)\n",
        "        open(path+'/'+nombre, \"wb\").write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiog1xryg91g"
      },
      "source": [
        "# Objetivos\n",
        "\n",
        "El propósito de este ejercicio es que el algoritmo que escribas, sea capaz de leer los documentos de la carpeta *Textos* que se descargan al ejecutar la celda superior, y mediante un *SOM* pueda clasificar cada uno de los documentos de manera automática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynOx7VjCg91h"
      },
      "source": [
        "## (3 Puntos) Clase `Documento`\n",
        "\n",
        "Genera una clase de *python* llamada `Documento` que contenga la información de un documento de texto. Esta clase debe tener los siguientes atributos:\n",
        "\n",
        "* `ruta`: Ruta del documento a procesar.\n",
        "* `texto`: Texto del documento, en minúsculas.\n",
        "* `vector_caracteristico`: Lista de frecuencias de las palabras del documento, de aceurdo con el `diccionario_universal`.\n",
        "\n",
        "Además, debe tener los siguientes métodos:\n",
        "\n",
        "* `__init__(self, ruta)`: Constructor de la clase. Recibe como parámetro la ruta del documento a procesar. Debe inicializar los atributos `ruta`, `texto` y `vector_caracteristico`.\n",
        "* `__str__(self)`: Método que devuelve la ruta del documento y su vector característico.\n",
        "* `preprocesar(self)`: Método que realiza el preprocesamiento del documento. Debe realizar las siguientes tareas:\n",
        "    * Leer el archivo de texto.\n",
        "    * Convertir el texto a minúsculas.\n",
        "* `data_mining(self)`: Método que debe contar la frecuencia de las palabras del documento, de acuerdo al `diccionario_universal` y almacenar el resultado en el atributo `vector_caracteristico`.\n",
        "\n",
        "Para que la clase `Documento` sea más \"ligera\", el `diccionario_universal` se define como una variable de clase. Esto quiere decir que es una variable que se comparte entre todas las instancias de la clase. Para definir una variable de clase, basta con definirla fuera de los métodos de la clase, pero dentro de la clase. Por ejemplo:\n",
        "\n",
        "```python\n",
        "class Documento:\n",
        "    diccionario_universal = ['palabra1', 'palabra2', 'palabra3']\n",
        "    \n",
        "    def __init__(self, ruta):\n",
        "        self.ruta = ruta\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRXd4W8Yg91h"
      },
      "source": [
        "## Diccionario universal\n",
        "\n",
        "Este diccionario es un conjunto de palabras que debe ser el resultado de analizar todos los documentos y obtener las palabras que aparecen en todos los documentos. De tal manera que este diccionario sirve para poder generar el vectori caracteristico de los documentos a procesar.\n",
        "\n",
        "Para fines prácticos, vamos a pensar que el diccionario universal contiene las siguientes palabras, en este orde:\n",
        "\n",
        "```python\n",
        "\n",
        "diccionario_universal = ['factura', 'testamento', 'demanda', 'contrato']\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpcgsGplg91h"
      },
      "outputs": [],
      "source": [
        "class Documento(object):\n",
        "    diccionario_universal = ['factura', 'testamento', 'demanda', 'contrato']\n",
        "\n",
        "    def __init__(self, ruta):\n",
        "        self.ruta = ruta\n",
        "        self.texto = \"\"\n",
        "        self.vector_caracteristico = None\n",
        "        self.preprocesar()\n",
        "        self.data_mining()\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'Vector característico: {self.vector_caracteristico}'\n",
        "\n",
        "    def preprocesar(self):\n",
        "        try:\n",
        "            with open(self.ruta, 'r') as archivo:\n",
        "                self.texto = archivo.read()\n",
        "\n",
        "            self.texto = self.texto.lower()\n",
        "        except Error_No_Se_Encontro_El_Archivo:\n",
        "            print(f\"El archivo en la ruta '{self.ruta}' no pudo ser encontrado.\")\n",
        "\n",
        "    def data_mining(self):\n",
        "        if not self.texto:\n",
        "            print(\"El texto del documento no está preprocesado.\")\n",
        "            return\n",
        "\n",
        "        self.vector_caracteristico = [0] * len(self.diccionario_universal)\n",
        "        palabras = self.texto.split()\n",
        "\n",
        "        for indice, palabra in enumerate(self.diccionario_universal):\n",
        "            self.vector_caracteristico[indice] = palabras.count(palabra)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kT335-tg91i"
      },
      "source": [
        "## Probando la clase `Documento`\n",
        "\n",
        "Una vez que se ha definido la clase `Documento`, se debe probar para verificar que funciona correctamente. Para ello, se debe crear una instancia de la clase `Documento` y llamar a los métodos `preprocesar` y `data_mining`. Por ejemplo:\n",
        "\n",
        "```python\n",
        "documento = Documento('./Textos/texto1.txt')\n",
        "print(documento.vector_caracteristico)\n",
        "```\n",
        "\n",
        "Y el resultado debe ser exactamente el siguiente:\n",
        "\n",
        "```\n",
        "[4, 0, 0, 0]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XJuurYKg91i",
        "outputId": "78d47f84-960b-4f99-d338-4f76788887fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "documento = Documento('./Textos/texto1.txt')\n",
        "print(documento.vector_caracteristico)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0TZz0Apg91i"
      },
      "source": [
        "## (2 Puntos) Clase `Clasificador`\n",
        "\n",
        "Genera una clase de *python* llamada `Clasificador` que genere una lista de objetos de la clase `Documento`. Esta clase debe tener los siguientes atributos:\n",
        "\n",
        "* `ruta`: Ruta de la carpeta que contiene los documentos a clasificar.\n",
        "* `documentos`: Lista de objetos de la clase `Documento`.\n",
        "\n",
        "Además, debe tener los siguientes métodos:\n",
        "\n",
        "* `__init__(self, ruta)`: Constructor de la clase. Recibe como parámetro la ruta de la carpeta que contiene los documentos a clasificar. Debe inicializar los atributos `ruta` y `documentos`.\n",
        "* `__str__(self)`: Método que devuelve la ruta de la carpeta y el número de documento a clasificar.\n",
        "* `cargar_documentos(self)`: Método que debe leer los documentos de la carpeta que se encuentre en `self.ruta` crear un objeto de tipo `Documento`, por cada documento dentro de la carpeta y almacenarlos en la lista `self.documentos`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKL9Vr3bg91i"
      },
      "outputs": [],
      "source": [
        "class Clasificador(object):\n",
        "    # YOUR CODE HERE\n",
        "    def __init__(self, ruta):\n",
        "            self.ruta = ruta\n",
        "            self.documentos = []\n",
        "\n",
        "    def __str__(self):\n",
        "        return '\\n'.join([f'Ruta: {doc.ruta}\\nVector caracteristico: {doc.vector_caracteristico}' for doc in self.documentos])\n",
        "\n",
        "    def cargar_documentos(self):\n",
        "        if os.path.exists(self.ruta) and os.path.isdir(self.ruta):\n",
        "            for filename in os.listdir(self.ruta):\n",
        "                if os.path.isfile(os.path.join(self.ruta, filename)):\n",
        "                    documento = Documento(os.path.join(self.ruta, filename))\n",
        "                    self.documentos.append(documento)\n",
        "        else:\n",
        "            print(f'La ruta \"{self.ruta}\" no es válida o no es un directorio.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfYizZD7g91j"
      },
      "source": [
        "## Probando la clase `Clasificador`\n",
        "\n",
        "Una vez que se ha definido la clase `Clasificador`, se debe probar para verificar que funciona correctamente. Para ello, se debe crear una instancia de la clase `Clasificador` y llamar al método `cargar_documentos`. Por ejemplo:\n",
        "\n",
        "```python\n",
        "clasificador = Clasificador('Textos')\n",
        "clasificador.cargar_documentos()\n",
        "print(clasificador)\n",
        "```\n",
        "\n",
        "Y el resultado debe ser exactamente el siguiente:\n",
        "\n",
        "```\n",
        "Ruta: Textos/texto1.txt\n",
        "Vector caracteristico: [4, 0, 0, 0]\n",
        "Ruta: Textos/texto2.txt\n",
        "Vector caracteristico: [2, 0, 0, 0]\n",
        "Ruta: Textos/texto3.txt\n",
        "Vector caracteristico: [10, 0, 0, 0]\n",
        "Ruta: Textos/texto4.txt\n",
        "Vector caracteristico: [0, 0, 0, 7]\n",
        "Ruta: Textos/texto5.txt\n",
        "Vector caracteristico: [0, 0, 0, 5]\n",
        "Ruta: Textos/texto6.txt\n",
        "Vector caracteristico: [0, 0, 0, 4]\n",
        "Ruta: Textos/texto7.txt\n",
        "Vector caracteristico: [0, 0, 4, 2]\n",
        "Ruta: Textos/texto8.txt\n",
        "Vector caracteristico: [0, 0, 2, 1]\n",
        "Ruta: Textos/texto9.txt\n",
        "Vector caracteristico: [0, 7, 0, 0]\n",
        "Ruta: Textos/texto10.txt\n",
        "Vector caracteristico: [0, 11, 0, 0]\n",
        "Ruta: Textos/texto11.txt\n",
        "Vector caracteristico: [0, 4, 0, 0]\n",
        "```\n",
        "\n",
        "Recuerda que la primer entrada de cada vector caracteristico es la frecuencia de la palabra `factura`, la segunda entrada es la frecuencia de la palabra `testamento`, la tercera entrada es la frecuencia de la palabra `demanda` y la cuarta entrada es la frecuencia de la palabra `contrato`.\n",
        "\n",
        "Asi que ya desde este momento puedes comenzar a pensar que clasificación le corresponde a cada documento. Sin embargo, para poder clasificar los documentos de manera automatica, primero se debe entrenar el SOM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CTLv_7g91j",
        "outputId": "83829e03-94a6-49f3-9a38-8e3918ca04f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruta: Textos/texto2.txt\n",
            "Vector caracteristico: [2, 0, 0, 0]\n",
            "Ruta: Textos/texto1.txt\n",
            "Vector caracteristico: [4, 0, 0, 0]\n",
            "Ruta: Textos/texto8.txt\n",
            "Vector caracteristico: [0, 0, 2, 1]\n",
            "Ruta: Textos/texto10.txt\n",
            "Vector caracteristico: [0, 11, 0, 0]\n",
            "Ruta: Textos/texto4.txt\n",
            "Vector caracteristico: [0, 0, 0, 7]\n",
            "Ruta: Textos/texto9.txt\n",
            "Vector caracteristico: [0, 7, 0, 0]\n",
            "Ruta: Textos/texto6.txt\n",
            "Vector caracteristico: [0, 0, 0, 4]\n",
            "Ruta: Textos/texto3.txt\n",
            "Vector caracteristico: [10, 0, 0, 0]\n",
            "Ruta: Textos/texto7.txt\n",
            "Vector caracteristico: [0, 0, 4, 2]\n",
            "Ruta: Textos/texto11.txt\n",
            "Vector caracteristico: [0, 4, 0, 0]\n",
            "Ruta: Textos/texto5.txt\n",
            "Vector caracteristico: [0, 0, 0, 5]\n"
          ]
        }
      ],
      "source": [
        "clasificador = Clasificador('Textos')\n",
        "clasificador.cargar_documentos()\n",
        "print(clasificador)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7c1cVwhg91j"
      },
      "source": [
        "## (2 Puntos) Documentos a clasificar\n",
        "\n",
        "Crea una lista de vectores caracteristicos de los documentos que se encuentran en la lista `clasificador.documentos`. Por ejemplo:\n",
        "\n",
        "```python\n",
        "vectores_caracteristicos = [documento.vector_caracteristico for documento in clasificador.documentos]\n",
        "```\n",
        "\n",
        "Esta lista debe verse exactamente como la siguiente:\n",
        "\n",
        "```\n",
        "[[4, 0, 0, 0], [2, 0, 0, 0], [10, 0, 0, 0], [0, 0, 0, 7], [0, 0, 0, 5], [0, 0, 0, 4], [0, 0, 4, 2], [0, 0, 2, 1], [0, 7, 0, 0], [0, 11, 0, 0], [0, 4, 0, 0]]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9aZiH3Rg91j"
      },
      "outputs": [],
      "source": [
        "vectores_caracteristicos = []\n",
        "\n",
        "# YOUR CODE HERE\n",
        "vectores_caracteristicos = [documento.vector_caracteristico for documento in clasificador.documentos]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7H7U-mcg91j"
      },
      "source": [
        "## (3 Puntos) Entrenando el SOM\n",
        "\n",
        "Crea una instancia de la clase `SOM` con un mapa de $2\\times2$ y con 4 entradas. Para ello, se debe crear una instancia de la clase `SOM`, por ejemplo:\n",
        "\n",
        "```python\n",
        "som = SOM(2, 2, 4, 10)\n",
        "\n",
        "```\n",
        "\n",
        "Ya con el SOM creado, se debe entrenar con los vectores caracteristicos de los documentos. Para ello, se debe llamar al método `train` del objeto `som`. Por ejemplo:\n",
        "\n",
        "```python\n",
        "som.train(vectores_caracteristicos)\n",
        "```\n",
        "\n",
        "Y finalmente, podemos ver la lista que nos devuelve el método `map_vects`. Por ejemplo:\n",
        "\n",
        "```python\n",
        "print(som.map_vects(vectores_caracteristicos))\n",
        "```\n",
        "\n",
        "El resultado der.\n",
        "\n",
        "```\n",
        "[array([1, 1]), array([1, 0]), array([1, 0]), array([0, 0]), array([0, 0]), array([1, 1]), array([0, 0]), array([0, 1]), array([0, 1]), array([1, 1]), array([0, 1])]\n",
        "```\n",
        "\n",
        "Esta es la clasificación que el SOM le ha dado a cada documento. Por ejemplo, el primer documento se encuentra en la posición `[1, 1]` del mapa, el segundo documento se encuentra en la posición `[1, 0]` del mapa, el tercer documento se encuentra en la posición `[1, 0]` del mapa, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGQCf-8rg91j",
        "outputId": "769cfe8d-ab56-4151-b326-5bc1e431f6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# EJECUTAR ESTA CELDA\n",
        "!pip install --user tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC8-dSbvg91k"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class SOM(object):\n",
        "    \"\"\"\n",
        "    Clase que representa una red neuronal tipo SOM.\n",
        "    \"\"\"\n",
        "\n",
        "    #Para revisar si la red ya ha sido entrenada\n",
        "    _trained = False\n",
        "\n",
        "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
        "        \"\"\"\n",
        "        Constructor que toma como parametros los valores descritos en el\n",
        "        algoritmo SOM. Genera un mapa de m renglones por n columnas y se entrenara\n",
        "        con n_iterations\n",
        "        \"\"\"\n",
        "\n",
        "        #Se inicializan variables que seran usadas a lo largo del coidgo\n",
        "        self._m = m\n",
        "        self._n = n\n",
        "        if alpha is None:\n",
        "            alpha = 0.3\n",
        "        else:\n",
        "            alpha = float(alpha)\n",
        "        if sigma is None:\n",
        "            sigma = max(m, n) / 2.0\n",
        "        else:\n",
        "            sigma = float(sigma)\n",
        "        self._n_iterations = abs(int(n_iterations))\n",
        "\n",
        "        '''SE NECESITA UNA GRAFICA (PLANO), hay una grafica\n",
        "        por default pero la guardamos en _graph'''\n",
        "        self._graph = tf.Graph()\n",
        "\n",
        "        '''SE CREAN LOS ELEMENTOS NECESARIOS EN LA GRAFICA'''\n",
        "        with self._graph.as_default():\n",
        "            '''SE CREAN TODAS LAS NEURONAS CON tf.Variable, son m*n\n",
        "            neuronas con dim pesos, que seran comparados con los pesos\n",
        "            de la entrada y la que tenga la menor distancia sera la\n",
        "            neurona ganadora. Antes de iniciar el entrenamiento, hay\n",
        "            hay que inicializar TODAS las variables'''\n",
        "\n",
        "            '''Lista de pesos de los vectores de la red neuronal'''\n",
        "            self._weightage_vects = tf.Variable(tf.random.normal(\n",
        "                [m*n, dim]))\n",
        "\n",
        "            '''Lista de 600 entradas, y cada entrada representa una\n",
        "            coordenada en la cual se encuentra cada neurona'''\n",
        "            self._location_vects = tf.constant(np.array(\n",
        "                list(self._neuron_locations(m, n))))\n",
        "\n",
        "            '''self._vect_input es un placeholder de tamano dim, ya que\n",
        "            es el objeto que sera alimentado con el vector de entrada y\n",
        "            a su vez este sera comparado con los pesos de cada neurona.\n",
        "            Esto es asi por el framework que da tensorflow'''\n",
        "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
        "\n",
        "            '''Lo mismo sucede con esta variable, la diferencia es que en\n",
        "            este punto aun no se sabe cuantas iteraciones (epocas) seran\n",
        "            necesarias, asi que se deja en cero.'''\n",
        "            self._iter_input = tf.placeholder(\"float\")\n",
        "\n",
        "            '''Devuelve el indice con el menor valor, es decir la neurona mas cercana.'''\n",
        "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
        "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
        "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
        "                                  0)\n",
        "\n",
        "            '''Variable que guarda el indice y un espacio para el sus\n",
        "            coordenada'''\n",
        "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
        "                                 np.array([[0, 1]]))\n",
        "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
        "                                          tf.constant(np.array([1, 2]))),\n",
        "                                 [2])\n",
        "\n",
        "            '''Valores necesario para actualizar los pesos de las neuronas\n",
        "            de acuerdo a la iteracion (epoca)'''\n",
        "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
        "                                                  self._n_iterations))\n",
        "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
        "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
        "\n",
        "            '''Calcula las distancias al cuadrado por cada neurona con respecto\n",
        "            a la neurona GANADORA (BMU). De tal manera que estos valores\n",
        "            puedan ser empleados para actualizar los pesos de los vecinos'''\n",
        "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
        "                self._location_vects, tf.stack(\n",
        "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
        "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
        "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
        "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
        "\n",
        "            '''Tasa de aprendizaje para actualizar los pesos de las neuronas'''\n",
        "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
        "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
        "                                               for i in range(m*n)])\n",
        "            weightage_delta = tf.multiply(\n",
        "                learning_rate_multiplier,\n",
        "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
        "                       self._weightage_vects))\n",
        "\n",
        "            '''Actualiza todos los pesos de las neuronas de acuerdo a los\n",
        "            parametros calculados previamente'''\n",
        "            new_weightages_op = tf.add(self._weightage_vects,\n",
        "                                       weightage_delta)\n",
        "\n",
        "            '''Se guarda la ultima operacion realizada en la SOM, ya que\n",
        "            esta operacion sera la que se ejecute y a su vez ejecuta todas\n",
        "            las operaciones previar al llamar a sess.run()'''\n",
        "            self._training_op = tf.assign(self._weightage_vects,\n",
        "                                          new_weightages_op)\n",
        "\n",
        "            '''En tensorflow todo debe ocurrir dentro de una sesion, es por\n",
        "            este motivo que se guarda la sesion'''\n",
        "            self._sess = tf.Session()\n",
        "\n",
        "            '''Forma en la tensorflow inicializa sus variables antes de ser\n",
        "            utilizadas'''\n",
        "            init_op = tf.initialize_all_variables()\n",
        "            self._sess.run(init_op)\n",
        "\n",
        "            '''centroid_grid es un mapa de bits en el cual se guardan los\n",
        "            valores de las neuronas. Es de tamano m, por que para cada renglon\n",
        "            se tienen n neuronas y sus respectivos valores. '''\n",
        "            centroid_grid = [[] for i in range(self._m)]\n",
        "            self._weightages = list(self._sess.run(self._weightage_vects))\n",
        "            self._locations = list(self._sess.run(self._location_vects))\n",
        "\n",
        "            '''Con este for, se accede a cada neurona por posicion y se guarda\n",
        "            en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
        "            ser facilmente graficado por matplotlib. Es el mapa incial (SIN ENTRENAR)'''\n",
        "            for i, loc in enumerate(self._locations):\n",
        "                centroid_grid[loc[0]].append(self._weightages[i])\n",
        "            self._mapa_inicial = centroid_grid\n",
        "\n",
        "    def _neuron_locations(self, m, n):\n",
        "        '''Yield regresa un generador flojo, y hasta que es necesario\n",
        "        se evalua. Esto se hace para que no haya informacion no necesaria\n",
        "        en memoria. En el constructor el resultado de esta funcion se\n",
        "        mete en una lista para que sea accesible de inmediato'''\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield np.array([i, j])\n",
        "\n",
        "    def train(self, input_vects, debbug=False):\n",
        "        if not debbug:\n",
        "            #centroid_grid = [[] for i in range(self._m)]\n",
        "            '''Para cada iteracion (epoca) se realiza el entrenamiento'''\n",
        "            for iter_no in range(self._n_iterations):\n",
        "                actual = self._sess.run(tf.norm(self._weightage_vects))\n",
        "                #Se entrena con cada vector uno por uno\n",
        "                for input_vect in input_vects:\n",
        "                    self._sess.run(self._training_op,\n",
        "                                  feed_dict={self._vect_input: input_vect,\n",
        "                                              self._iter_input: iter_no})\n",
        "                siguiente = self._sess.run(tf.norm(self._weightage_vects))\n",
        "                '''Si la norma del mapa actual no varia mucho con respecto\n",
        "                al siguiente, se rompe el ciclo de las epocas'''\n",
        "                if abs(siguiente - actual) <= 0.000001:\n",
        "                    break\n",
        "            '''centroid_grid es un mapa de bits en el cual se guardan los\n",
        "                valores de las neuronas. Es de tamano m, por que para cada renglon\n",
        "                se tienen n neuronas y sus respectivos valores. '''\n",
        "            centroid_grid = [[] for i in range(self._m)]\n",
        "            self._weightages = list(self._sess.run(self._weightage_vects))\n",
        "            self._locations = list(self._sess.run(self._location_vects))\n",
        "\n",
        "            '''Con este for, se accede a cada neurona por posicion y se guarda\n",
        "                en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
        "                ser facilmente graficado por matplotlib. En este punto la red ya esta entrenada.'''\n",
        "            for i, loc in enumerate(self._locations):\n",
        "                centroid_grid[loc[0]].append(self._weightages[i])\n",
        "            self._centroid_grid = centroid_grid\n",
        "\n",
        "            '''En este punto la red ya esta entrenada.'''\n",
        "            self._trained = True\n",
        "            '''Esta seccion muestra como se entrena el SOM y es basicamente el mismo\n",
        "            codigo de la seccion del if y al final solo se agrega la grafica del mapa.'''\n",
        "        else:\n",
        "            centroid_grid = [[] for i in range(self._m)]\n",
        "\n",
        "            for iter_no in range(self._n_iterations):\n",
        "                actual = self._sess.run(tf.norm(self._weightage_vects))\n",
        "                #Se entrena con cada vector uno por uno\n",
        "                for input_vect in input_vects:\n",
        "                    self._sess.run(self._training_op,\n",
        "                                  feed_dict={self._vect_input: input_vect,\n",
        "                                              self._iter_input: iter_no})\n",
        "                siguiente = self._sess.run(tf.norm(self._weightage_vects))\n",
        "\n",
        "                if abs(siguiente - actual) <= 0.000001:\n",
        "                    break\n",
        "                if iter_no % 10 == 0:\n",
        "                    centroid_grid = [[] for i in range(self._m)]\n",
        "                    self._weightages = list(self._sess.run(self._weightage_vects))\n",
        "                    self._locations = list(self._sess.run(self._location_vects))\n",
        "\n",
        "                    for i, loc in enumerate(self._locations):\n",
        "                        centroid_grid[loc[0]].append(self._weightages[i])\n",
        "                    self._centroid_grid = centroid_grid\n",
        "\n",
        "                    red_entrenada = som.get_centroids()\n",
        "                    # SECCION PARA GRAFICAR\n",
        "                    bmu = self.map_vect(input_vect)\n",
        "                    plt.text(bmu[1], bmu[0], \"bmu\", ha='center', va='center',\n",
        "                        bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "                    plt.imshow(red_entrenada)\n",
        "                    plt.show()\n",
        "                    input(\"Continuar?\")\n",
        "\n",
        "            '''En este punto la red ya esta entrenada.'''\n",
        "            self._trained = True\n",
        "\n",
        "    def get_centroids(self):\n",
        "        # Solo devuelve los centroides para que puendan ser graficados\n",
        "        #if not self._trained:\n",
        "            #raise ValueError(\"La red aun no ha sido entrenada\")\n",
        "        return self._centroid_grid\n",
        "\n",
        "    def map_vects(self, input_vects):\n",
        "        '''to_return es la lista que contiene las coordenadas (x,y) de la\n",
        "        neurona que mas se parece a cada una de las entradas de input_vects\n",
        "        en el mismo orden'''\n",
        "\n",
        "        if not self._trained:\n",
        "            raise ValueError(\"SOM not trained yet\")\n",
        "\n",
        "        to_return = []\n",
        "        for vect in input_vects:\n",
        "            min_index = min([i for i in range(len(self._weightages))],\n",
        "                            key=lambda x: np.linalg.norm(vect-\n",
        "                                                         self._weightages[x]))\n",
        "            to_return.append(self._locations[min_index])\n",
        "\n",
        "        return to_return\n",
        "\n",
        "    def map_vect(self, vect):\n",
        "        '''\n",
        "        Mapea un solo vector y devuelve la clasificacion vista como\n",
        "        un indice relacionado a la coordenada (x,y) de la neurona\n",
        "        '''\n",
        "\n",
        "        min_index = min([i for i in range(len(self._weightages))],\n",
        "                        key=lambda x: np.linalg.norm(\n",
        "                            vect - self._weightages[x]))\n",
        "        pos2D = self._locations[min_index]\n",
        "        # polinomio de direccionamiento de la neurona\n",
        "        #return pos2D[0]*self._m + pos2D[1], pos2D\n",
        "        return (pos2D[1], pos2D[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu9e0w45g91k",
        "outputId": "b1fd8ecd-7f28-4436-9c37-1a16769fd1c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([0, 0]), array([0, 1]), array([0, 1]), array([1, 1]), array([0, 0]), array([1, 1]), array([1, 0]), array([0, 0]), array([1, 1]), array([1, 0]), array([1, 0])]\n"
          ]
        }
      ],
      "source": [
        "# Crear una instancia de la clase SOM con un mapa de 2x2 y 4 entradas\n",
        "som = SOM(2, 2, 4, 10)\n",
        "\n",
        "# Entrenar el SOM con los vectores característicos de los documentos\n",
        "som.train(vectores_caracteristicos)\n",
        "\n",
        "# Obtener la clasificación de los documentos utilizando el método map_vects\n",
        "clasificacion_documentos = som.map_vects(vectores_caracteristicos)\n",
        "\n",
        "# Imprimir la clasificación\n",
        "print(clasificacion_documentos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuT9fZgdg91k"
      },
      "source": [
        "Dada la naturaleza aleatoria inicial del mapa, es posible que tu clasificación sea diferente a la que se muestra arriba. Sin embargo, si se ha hecho todo correctamente, la clasificación debe ser muy similar a la que se muestra arriba.\n",
        "\n",
        "Finalmente mediante el polinomio de direccionamiento, se puede \"aplanar\" el mapa y obtener una lista de clasificaciones. Por ejemplo:\n",
        "\n",
        "```python\n",
        "clasificaciones = [0,0,0,0]\n",
        "for clas in mapeados:\n",
        "    # polinomio de direccionamiento para aplanar el mapa\n",
        "    clasificaciones[clas[0]*som._m + clas[1]] += 1\n",
        "print(clasificaciones)\n",
        "```\n",
        "\n",
        "Si ordenas la lista de clasificaciones de mayor a menor y todo funciona de manera correcta el resultado que verás será el siguiente:\n",
        "\n",
        "```python\n",
        "[3, 3, 3, 2]    \n",
        "```\n",
        "\n",
        "Lo que significa que tenemos 3 documentos clasificados como `contrato`, 3 documentos clasificados como `factura`, 3 documentos clasificados como `testamento` y 2 documentos clasificados como `demanda`. Si no me crees, puedes revisar manualmente cada documento y verificar que la clasificación es correcta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkViP_9ug91k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033ff13c-e1a0-4fb3-f3bb-03ecf0ecb68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 3, 3, 2]\n"
          ]
        }
      ],
      "source": [
        "clasificaciones = [0,0,0,0]\n",
        "# YOUR CODE HERE\n",
        "clasificaciones = [0, 0, 0, 0]\n",
        "\n",
        "for clas in clasificacion_documentos:\n",
        "    # Polinomio de direccionamiento para aplanar el mapa\n",
        "    index = clas[0] * som._m + clas[1]\n",
        "    clasificaciones[index] += 1\n",
        "\n",
        "    clasificaciones_ordenadas = sorted(clasificaciones, reverse = True)\n",
        "\n",
        "print(clasificaciones_ordenadas)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}